{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Segmentation and Fluorescence Extraction\n",
    "\n",
    "This notebook performs cell segmentation and extracts fluorescence information for individual cells.\n",
    "\n",
    "Therfore, the following steps will be performed:\n",
    "\n",
    "1. Perform segmentation on the phase-contrast image\n",
    "2. Extract individual cell information\n",
    "3. Filter cells based on there individual information to reduce the number of artifacts\n",
    "4. Extract single-cell fluorescence information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "parameters"
    ]
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "\n",
    "# image sequence id to analyze\n",
    "input_image_file = \"data/17415.tif\"\n",
    "\n",
    "# we make sure the output path exists\n",
    "output_path = Path(\"./\") / \"tmp/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_path = Path(output_path)\n",
    "output_path.mkdir(exist_ok=True)\n",
    "\n",
    "input_image_file = Path(input_image_file)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Cell Segmentation\n",
    "\n",
    "We specify the phase-contrast channel and perform the segmentation using [Omnipose](https://www.nature.com/articles/s41592-022-01639-4)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from acia.segm.processor import FlexibleSegmentationModel, ModelDescriptor\n",
    "from acia.segm.local import LocalSequenceSource\n",
    "from acia.segm.local import LocalImage\n",
    "\n",
    "phase_contrast_channel = 1\n",
    "\n",
    "# the model description\n",
    "model_desc = ModelDescriptor(\n",
    "    repo=\"https://gitlab+deploy-token-233:aoDtsfRDM1gXwj9cyXJz@jugit.fz-juelich.de/mlflow-executors/cellpose-executor.git\",\n",
    "    entry_point=\"omnipose\",\n",
    "    version=\"14-change-mask-parser\"\n",
    ")\n",
    "\n",
    "# connect to remote machine learning model\n",
    "model = FlexibleSegmentationModel(model_desc, batch_size=30)\n",
    "\n",
    "# create local image data source\n",
    "source = LocalSequenceSource(input_image_file)\n",
    "\n",
    "# perform overlay prediction\n",
    "print(\"Perform Prediction...\")\n",
    "result = model.predict(map(lambda i: LocalImage(i.raw[phase_contrast_channel]),source))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To validate the segmentation result, we create a short video:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import acia\n",
    "from acia.segm.output import renderVideo\n",
    "import numpy as np\n",
    "\n",
    "import numpy as np\n",
    "rgb_images = list(map(lambda i: LocalImage(np.stack([i.raw[1], i.raw[1], i.raw[1]], axis=-1)),source))\n",
    "\n",
    "framerate=2\n",
    "\n",
    "# Make a video with\n",
    "video_file = str(output_path / \"segmented.mp4\")\n",
    "renderVideo(rgb_images, result.timeIterator(), filename=video_file, codec=\"vp09\", framerate=framerate, draw_frame_number=True)\n",
    "\n",
    "# display markdown\n",
    "from IPython.display import Video, Markdown, display\n",
    "display(Markdown(\"# Your segmentation\"))\n",
    "Video(video_file, embed=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "# 2. Extracting individual cell properties\n",
    "\n",
    "Now that we have the cell segmentation, we can move on and extract individual cell properties like Area, Time, Length, ....\n",
    "and visualize them in a table:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "flour_images = list(map(lambda i: LocalImage(i.raw[0]),source))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from acia.analysis import ExtractorExecutor, AreaEx, IdEx, FrameEx, TimeEx, FluorescenceEx, PositionEx\n",
    "from acia import ureg\n",
    "import pint\n",
    "import numpy as np\n",
    "from acia.segm.local import InMemorySequenceSource\n",
    "\n",
    "# fluorescence channel is the first one\n",
    "fluorescence_channel = 0\n",
    "\n",
    "# create local image data source\n",
    "#source = OmeroSequenceSource(image_id, **credentials, channels=[fluorescence_channel], colorList=['FF0000'])  # render fluorescence channel into first channel of the image\n",
    "\n",
    "raw_source = InMemorySequenceSource(list(map(lambda i: i.raw[0], LocalSequenceSource(input_image_file, normalize_image=False))))\n",
    "\n",
    "ex = ExtractorExecutor()\n",
    "\n",
    "pixel_size = 0.07335\n",
    "\n",
    "df = ex.execute(result, raw_source, [\n",
    "    # define the cell properties that you want to extract here\n",
    "    AreaEx(input_unit=(1 * ureg.pixel) ** 2, output_unit=ureg.pixel**2),  # pass the correct area of pixels\n",
    "    IdEx(),\n",
    "    FrameEx(),\n",
    "    PositionEx(input_unit=pixel_size * ureg.micrometer),\n",
    "    TimeEx(input_unit=\"5 * minute\"),  # one picture every 5 minutes\n",
    "    FluorescenceEx(channels=[fluorescence_channel], channel_names=[\"gfp\"], summarize_operator=np.sum, parallel=1)\n",
    "])\n",
    "\n",
    "print(df)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "# 3. Filtering artifacts in segmentation\n",
    "\n",
    "In the segmentation, we can often observe artifacts, that is objects that are mistakenly recoginzed as cells. To reduce the number of artifacts in our analysis we can utilize some simple filtering functionality for the area and the borders: We only keep all the objects that have an area between `min_area` and `max_area` and are further from the border than `margin` as defined below in the code ðŸ‘‡"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "min_area = 0.7  # the minimal area in micrometer ** 2. All smaller objects are dropped\n",
    "max_area = 2000 # the maximal area in micrometer ** 2. All larger objects are dropped\n",
    "\n",
    "fig, ax = plt.subplots(2, 1, facecolor='white', figsize=(15,10))\n",
    "\n",
    "area_unit = ex.units['area']\n",
    "\n",
    "# plot the area distribution before filtering\n",
    "ax[0].hist(df['area'], bins=100)\n",
    "ax[0].set_title('Area distribution before filtering')\n",
    "ax[0].set_ylabel('Frequency')\n",
    "ax[0].set_xlabel(f'Cell area [${area_unit:~L}$]')\n",
    "\n",
    "# cell cemter should at least be .5 micrometer away from border\n",
    "margin = .5\n",
    "\n",
    "img = source.get_frame(0).raw\n",
    "\n",
    "left, top = 0,0\n",
    "bottom, right = np.array(img.shape[-2:]) * pixel_size\n",
    "\n",
    "# filter by area and border\n",
    "filtered_df = df[(min_area < df['area']) & (df['area'] < max_area) & ~(df[\"position_x\"] < margin) & ~(df[\"position_x\"] > right - margin) & ~(df[\"position_y\"] < margin) & ~(df[\"position_y\"] > bottom - margin)]\n",
    "\n",
    "# plot the area distribution after filtering\n",
    "ax[1].hist(filtered_df['area'], bins=100)\n",
    "ax[1].set_title('Area distribution after filtering')\n",
    "ax[1].set_ylabel('Frequency')\n",
    "ax[1].set_xlabel(f'Cell area [${area_unit:~L}$]')\n",
    "\n",
    "plt.tight_layout()\n",
    "\n",
    "print(\"Done\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And now let's look at the new video with filtered content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make a video with\n",
    "video_file = str(output_path / \"filter_segmented.mp4\")\n",
    "renderVideo(rgb_images, result.timeIterator(), filename=video_file, codec=\"vp09\", framerate=framerate, draw_frame_number=True, filter_contours=lambda i,c: c.id in filtered_df['id'])\n",
    "\n",
    "# display markdown\n",
    "from IPython.display import Video, Markdown, display\n",
    "display(Markdown(\"# Your segmentation\"))\n",
    "Video(video_file, embed=False)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "# 4. Visualizing interesting properties\n",
    "\n",
    "We start with preparing the units"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from acia import ureg\n",
    "from pint import set_application_registry\n",
    "import pint_pandas\n",
    "\n",
    "set_application_registry(ureg)\n",
    "\n",
    "if not \"fluorescence\" in ureg:\n",
    "    # define fluorescence unit\n",
    "    ureg.define(\"fluorescence = [au] = fluor = fluorescence\")\n",
    "    \n",
    "ureg.define(\"pixe = 0.07335 * micrometer\")\n",
    "# 1 pixel = 0.07335 um\n",
    "\n",
    "# give units to the pandas array\n",
    "unit_df = filtered_df.astype({'area': f\"pint[{ex.units['area']}]\", 'position_x': f\"pint[{ex.units['position_x']}]\", 'position_y': f\"pint[{ex.units['position_y']}]\",  'gfp': f\"pint[fluorescence]\", 'time': f\"pint[hr]\", 'id': \"pint[dimensionless]\", 'frame': \"pint[dimensionless]\"})\n",
    "\n",
    "print(unit_df.dtypes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4.1 Visualizing fluorescence response\n",
    "\n",
    "Here we want to visualize the average flourescence response per frame (total colony gfp / colony area [a.u./um^2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute the average GFP of each cell (GFP = mean gfp * area)\n",
    "unit_df['mean gfp / area'] = unit_df['gfp'] / unit_df['area']\n",
    "\n",
    "# export with german decimal: ,\n",
    "unit_df.pint.dequantify().to_csv(str(output_path / 'allcells.csv'), decimal='.', sep=';')\n",
    "\n",
    "# compute the sum of GFP and area for each frame\n",
    "sum_df = unit_df.groupby(['frame', 'time'], as_index=False).sum()\n",
    "\n",
    "# compute the fluorescence per area for every frame\n",
    "sum_df['sum(GFP) / sum(area)'] = sum_df['gfp'] / sum_df['area']\n",
    "\n",
    "# export with german decimal: ,\n",
    "sum_df.pint.dequantify().to_csv(str(output_path / 'sum.csv'), decimal='.', sep=';')\n",
    "\n",
    "# visualize average gfp response over time\n",
    "plt.figure(facecolor='white', figsize=(15,10))\n",
    "plt.plot(sum_df['time'].pint.magnitude, sum_df['sum(GFP) / sum(area)'].pint.to(\"fluor / pixel ** 2\").pint.magnitude)\n",
    "plt.title(r'GFP per $pixel^2$ over time', fontsize=25)\n",
    "plt.xlabel(f'Time [${unit_df[\"time\"].pint.u:~L}$]', fontsize=20)\n",
    "plt.ylabel(f'GFP [${sum_df[\"sum(GFP) / sum(area)\"].pint.to(\"fluor / pixel ** 2\").pint.u:~L}$]', fontsize=20)\n",
    "plt.savefig(output_path /'GFP_per_um^2_over_time.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# write to csv including units\n",
    "sum_df.pint.dequantify().to_csv(str(output_path / 'sum.csv'), sep=\";\", decimal=\",\")"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "43e720662e2b73f3f858656968524fca68eb44fc0b1d15b9eb878c7d185562f9"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
